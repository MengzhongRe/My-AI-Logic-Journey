from tabnanny import check
from transformers import AutoTokenizer,AutoModelForMaskedLM,set_seed
import torch

def main():
    checkpoint = 'bert-base-chinese'
    print(f'æ­£åœ¨åŠ è½½ {checkpoint} æ¨¡å‹å’Œåˆ†è¯å™¨...')
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForMaskedLM.from_pretrained(checkpoint).to(device)

    text = '.'
    inputs = tokenizer(text,return_tensors='pt').to(device)
    print(f'ç»™å®šå¥å­ï¼š {text}')
    print(f'input_ids: {inputs['input_ids']}')
    with torch.no_grad():
        logits = model(**inputs).logits
    
    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

    # 8. è·å–é¢„æµ‹ç»“æœ
    # å–å‡º [MASK] ä½ç½®çš„é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªè¯çš„ ID
    predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
    
    # è§£ç å›å•è¯
    predicted_word = tokenizer.decode(predicted_token_id)
    
    print(f"ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ: {predicted_word}")
    print(f"å®Œæ•´å¥å­: {text.replace('[MASK]', predicted_word)}")

if __name__ == "__main__":
    main()
